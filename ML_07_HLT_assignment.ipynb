{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLT Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to practising the process of loading data, regularizing it and using it to train a model, the goal of this assignment is to investigat the effect of changing model parameters on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be going a bit further with the robot collision dataset. This time, instead of looking at just the first file, we'll look at all five different tasks combined (lp1.data to lp5.data). Prepare two different arrays, X1 and X2, as follows:\n",
    "- Each element in X1 is the immediate reading of the force and torque values after an event, [f1, f2, f3, t1, t2, t3]. The first element should be [1, 1, 63, 3, 1, 0]\n",
    "- Each element in X2 contains 18 values in total - the first, fifth and tenth sets of sensor readings after an event. The first element should be [-1, -1, 61, -3, 0, 0, -1, -1, 63, -3, -1, 0, -1, -1, 61, -3, 0, 0]\n",
    "\n",
    "y should contain the corresponding classes, represented as integers according the the provided dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['robot_execution_failure/lp1.data', 'robot_execution_failure/lp2.data',\n",
    "'robot_execution_failure/lp3.data', 'robot_execution_failure/lp4.data', 'robot_execution_failure/lp5.data']\n",
    "classes = {'normal':0, 'collision':1, 'obstruction':2, 'fr_collision':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('robot_execution_failure/lp1.data'); f2= open('robot_execution_failure/lp2.data'); f3= open('robot_execution_failure/lp3.data');\n",
    "f4= open( 'robot_execution_failure/lp4.data'); f5= open('robot_execution_failure/lp5.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal\\n',\n",
       " '\\t-1\\t-1\\t63\\t-3\\t-1\\t0\\n',\n",
       " '\\t0\\t0\\t62\\t-3\\t-1\\t0\\n',\n",
       " '\\t-1\\t-1\\t61\\t-3\\t0\\t0\\n',\n",
       " '\\t-1\\t-1\\t63\\t-2\\t-1\\t0\\n']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = f.readlines()\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal\\n',\n",
       " '\\t-2\\t-1\\t81\\t0\\t-5\\t0\\n',\n",
       " '\\t-2\\t-1\\t79\\t0\\t-4\\t0\\n',\n",
       " '\\t-2\\t-1\\t79\\t0\\t-4\\t0\\n',\n",
       " '\\t-2\\t-1\\t80\\t0\\t-4\\t0\\n']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines2 = f2.readlines()\n",
    "lines2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ok\\n',\n",
       " '\\t-2\\t-1\\t81\\t0\\t-5\\t0\\n',\n",
       " '\\t-2\\t-1\\t79\\t0\\t-4\\t0\\n',\n",
       " '\\t-2\\t-1\\t79\\t0\\t-4\\t0\\n',\n",
       " '\\t-2\\t-1\\t80\\t0\\t-4\\t0\\n']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines3 = f3.readlines()\n",
    "lines3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal\\n',\n",
       " '\\t-2\\t2\\t20\\t5\\t-6\\t-1\\n',\n",
       " '\\t-2\\t1\\t20\\t5\\t-6\\t0\\n',\n",
       " '\\t-2\\t1\\t23\\t5\\t-6\\t0\\n',\n",
       " '\\t-2\\t2\\t20\\t5\\t-6\\t-1\\n']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines4 = f4.readlines()\n",
    "lines4[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal\\n',\n",
       " '\\t-2\\t-1\\t81\\t0\\t-5\\t0\\n',\n",
       " '\\t-2\\t-1\\t79\\t0\\t-4\\t0\\n',\n",
       " '\\t-2\\t-1\\t79\\t0\\t-4\\t0\\n',\n",
       " '\\t-2\\t-1\\t80\\t0\\t-4\\t0\\n']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines5 = f5.readlines()\n",
    "lines5[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: establishing a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using techniques covered in this unit, split X1 and y into separate training and testing sets. Use the training set to train a neural network (MLPClassifier) using default parameters but with hidden_layer_sizes=(20, 20, 20). Use the test data you held back to score the model you have created. How well does it perform? Print out the score and confusion matrix. For more accuracy, run through these steps 10 times and find the average score - bonus points for running more times and getting a standard deviation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For lp1 data, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "y = []\n",
    "\n",
    "for i in range(len(lines) - 1):\n",
    "    line = lines[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X.append(features)\n",
    "        y.append(classes[line]) # And record which class this set of features belongs to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#### For lp2 data, file2\n",
    "\n",
    "X2 = [] \n",
    "y2 = []\n",
    "\n",
    "for i in range(len(lines2) - 1):\n",
    "    line = lines2[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines2[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X2.append(features)\n",
    "        y2.append(classes[line]) # And record which class this set of features belongs to\n",
    "\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2)\n",
    "\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#### For lp3 data, file3\n",
    "\n",
    "X3 = [] \n",
    "y3 = []\n",
    "\n",
    "for i in range(len(lines3) - 1):\n",
    "    line = lines3[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines3[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X3.append(features)\n",
    "        y3.append(classes[line]) # And record which class this set of features belongs to\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y3)\n",
    "\n",
    "print(len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "#### For lp4 data, file3\n",
    "\n",
    "X4 = [] \n",
    "y4 = []\n",
    "\n",
    "for i in range(len(lines4) - 1):\n",
    "    line = lines4[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines4[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X4.append(features)\n",
    "        y4.append(classes[line]) # And record which class this set of features belongs to\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X4, y4)\n",
    "\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "#### For lp4 data, file3\n",
    "\n",
    "X5 = [] \n",
    "y5 = []\n",
    "\n",
    "for i in range(len(lines5) - 1):\n",
    "    line = lines5[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines5[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X5.append(features)\n",
    "        y5.append(classes[line]) # And record which class this set of features belongs to\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X5, y5)\n",
    "\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine X1 and y1\n",
    "X1 = X + X2 + X3 + X4 + X5 \n",
    "y1 = y + y2 + y3 + y4 + y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "predictions = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're getting a convergence warning, you could try having the model train over more iterations - change max_iter = 1000 or 10,000. Does this improve the average score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: adding more inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use X2 in place of X1 - does the score increase or decrease? Was this what you expected? How many samples are there in our training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to X1 as our input. Add an extra feature to each item in the array to represent the total force $f_t$. Assume:\n",
    "\n",
    "$f_t^2 = f_1^2 + f_2^2 + f_3^2$\n",
    "\n",
    "Your first input should now look like this:\n",
    "X1[0] = [-1, -1, 61, -3, 0, 0, 61.0163912403872]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the steps from step 2. *Has this extra feature improved model performance?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-95-06e4bff3babd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-95-06e4bff3babd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    X1[0]=\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create at least 3 more models, adding features or changing the number and size of the hidden layers. Print out the average score for your best model. Comment on what you've found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
